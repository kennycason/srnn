# SRNN: Saturation Routing Neural Network

A Mixture-of-Experts (MoE) autoencoder with **gradient-based saturation routing** that enables progressive expert growth without collapse.

## The Problem

Standard MoE routing suffers from **expert collapse** during progressive growth:
- Add new experts â†’ they have random weights â†’ perform poorly
- Router learns to avoid them â†’ they get no gradients â†’ never improve
- Death spiral: new experts are permanently ignored

## The Solution: Saturation Routing

Track expert "saturation" via gradient magnitude and bias routing toward "hungry" (less-saturated) experts.

### The Math

**1. Measure gradient magnitude after each backward pass:**

```
grad_mag[i] = mean(|âˆ‚Loss/âˆ‚Î¸áµ¢|)  for all parameters Î¸ in expert i
```

**2. Maintain exponential moving average (EMA):**

```
saturation[i] = Î± Â· saturation[i] + (1-Î±) Â· grad_mag[i]

where Î± = 0.9 (decay factor)
```

**3. Bias router logits toward hungry experts:**

```
biased_logits = logits + Î² Â· (1 - saturation)

where Î² = 0.5 (bias strength)
```

### Interpretation

| Gradient Magnitude | Saturation | Router Bias | Meaning |
|-------------------|------------|-------------|---------|
| HIGH | HIGH | LOW | Expert is actively learning, getting traffic |
| LOW | LOW | HIGH | Expert is idle, router sends more traffic |

When new experts are added, they start with medium saturation (0.5) and receive biased routing, giving them a chance to learn.

## Architecture

```
Input Image (64Ã—64Ã—3)
       â†“
   ConvEncoder â”€â”€â†’ Skip Connections â”€â”€â”
       â†“                              â”‚
   Latent z (128-dim)                 â”‚
       â†“                              â”‚
   SaturationRouter                   â”‚
       â†“                              â”‚
   Top-K Expert Selection             â”‚
       â†“                              â”‚
   Weighted Expert Outputs            â”‚
       â†“                              â”‚
   ConvDecoder â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
   Reconstruction (64Ã—64Ã—3)
```

**Components:**
- **ConvEncoder**: 4 conv blocks (32â†’64â†’128â†’256 channels), outputs latent + skip connections
- **ConvDecoder**: U-Net style with skip connections for sharp reconstructions
- **Expert**: Small MLP with residual connection (z + MLP(z))
- **SaturationRouter**: Linear projection + gradient-based saturation bias

## Results

We compared three training approaches on 100K images from Tiny ImageNet:

| Experiment | Approach | Active Experts | CV (â†“ better) | Loss |
|------------|----------|----------------|---------------|------|
| Fixed Capacity | 16 experts, 40 epochs, replay | **16/16** | **0.184** | 0.000112 |
| Progressive | 2â†’16 experts, no replay | 14/16 | 1.404 | 0.001117 |
| Two-Phase | 4â†’16 experts, no replay | **16/16** | 1.069 | 0.000445 |

### Key Findings

1. **Saturation routing prevents collapse**: Without it, progressive growth collapses to ~3/16 active experts. With it, we maintain **87-100% utilization**.

2. **No replay needed**: Two-Phase achieves full 16/16 utilization without seeing any data twice â€” enabling true continual learning.

3. **Trade-offs**:
   - Fixed: Best balance but requires replay
   - Progressive: Fastest but slight utilization drop
   - Two-Phase: Best for continual learning scenarios

![Comparison](results/comparison/metrics_comparison.png)

ğŸ“Š **[See detailed results and analysis â†’](RESULTS.md)**

## Experiments

### Demo (`experiments/demo.py`)
Quick training on 10K images to verify setup and visualize reconstructions.

### Experiment 1: Fixed Capacity (`experiments/exp1_fixed.py`)
- 16 experts from the start
- Train on all 100K images with replay (multiple epochs)
- Baseline for comparison

### Experiment 2: Progressive Growth (`experiments/exp2_progressive.py`)
- Start with 2 experts
- Add 1 expert every ~7K images (reaching 16 at 100K)
- **No replay** - each image seen only once
- Tests catastrophic forgetting and expert specialization

### Experiment 3: Two-Phase (`experiments/exp3_two_phase.py`)
- Phase 1: Train on first 10K images with initial experts
- Phase 2: Add experts, train on remaining 90K (no replay)
- Simpler version of progressive growth

## Installation

```bash
git clone https://github.com/yourusername/srnn.git
cd srnn
pip install -r requirements.txt
```

## Usage

### Quick Demo

```python
from srnn import ConvEncoder, ConvDecoder, MoEAutoencoder

# Create model
encoder = ConvEncoder(latent_dim=128)
decoder = ConvDecoder(latent_dim=128)
model = MoEAutoencoder(
    encoder, decoder,
    num_experts=4,
    latent_dim=128,
    top_k=2,
    saturation_ema=0.9,
    capacity_bias=0.5,
)

# Forward pass
x_hat, gate, topk_idx = model(images)

# After loss.backward(), update saturation
model.update_saturation()

# Add experts dynamically
model.add_expert()  # Add one
model.add_experts(4)  # Add multiple
```

### Run Experiments

```bash
# Quick demo
python experiments/demo.py

# Full experiments
python experiments/exp1_fixed.py
python experiments/exp2_progressive.py
python experiments/exp3_two_phase.py
```

## Key Files

```
srnn/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ models.py          # Encoder, Decoder, Expert, MoEAutoencoder
â”œâ”€â”€ router.py          # SaturationRouter with gradient tracking
â”œâ”€â”€ training.py        # Training loops and metrics
â”œâ”€â”€ analysis.py        # Visualization utilities
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ demo.py
â”‚   â”œâ”€â”€ exp1_fixed.py
â”‚   â”œâ”€â”€ exp2_progressive.py
â”‚   â”œâ”€â”€ exp3_two_phase.py
â”‚   â””â”€â”€ compare_all.py # Generate comparison visualizations
â”œâ”€â”€ results/           # Saved experiment results
â”‚   â”œâ”€â”€ comparison/    # Cross-experiment comparisons
â”‚   â”œâ”€â”€ demo/
â”‚   â”œâ”€â”€ exp1_fixed/
â”‚   â”œâ”€â”€ exp2_progressive/
â”‚   â””â”€â”€ exp3_two_phase/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ RESULTS.md         # Detailed experimental analysis
â””â”€â”€ CONTEXT.md         # Development notes
```

## Hyperparameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `latent_dim` | 128 | Dimension of latent space |
| `top_k` | 2 | Number of experts selected per input |
| `saturation_ema` | 0.9 | EMA decay for saturation tracking |
| `capacity_bias` | 0.5 | Strength of routing bias toward hungry experts |
| `load_balance_weight` | 0.1 | Weight of auxiliary load balancing loss |

## Citation

If you find this useful, please cite:

```
@misc{srnn2024,
  title={SRNN: Saturation Routing for Progressive Expert Growth},
  author={Your Name},
  year={2024},
  url={https://github.com/yourusername/srnn}
}
```

## License

MIT

